{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0501f5b9-313d-4422-9d97-aa7bd76e635d",
   "metadata": {},
   "source": [
    "## Basic Neural Network Architecture \n",
    "\n",
    "Macro components `slt`:\n",
    "- __stem__ takes the input and does initial coarse-level feature extraction\n",
    "- __learner__ is composed of any number of convolutional groups. Does detailed feature extraction and convolutional learning from teh extracted coarse feature.\n",
    "- __task__ learns the task (e.g. classification) from the representation of the input in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cb6a0f-7498-4548-b60b-492ae78bd600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stem(input_layers):\n",
    "    \"\"\"stem layers\n",
    "    input_shape: the shape of the input tensor\n",
    "    \"\"\"\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def learner(inputs):\n",
    "    \"\"\"learner layers\n",
    "    inputs: the input tensors (feature maps)\n",
    "    \"\"\"\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def task(inputs, n_classes):\n",
    "    \"\"\"classifier layers\n",
    "    inputs: the input tensors (feature maps)\n",
    "    n_classes: the number of output classes\n",
    "    \"\"\"\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46ee6bbf-e37c-4e76-8aed-afcd806947c2",
   "metadata": {},
   "source": [
    "inputs = Input(input_shape=(224,224,3)) # A 224x224 RGB image.\n",
    "outputs = stem(inputs)\n",
    "outputs = learner(outputs)\n",
    "outputs = task(x, n_classes=1000)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e7ec6-e5c0-498a-9cae-13785b84cf95",
   "metadata": {},
   "source": [
    "## Stem component\n",
    "\n",
    "- the entry point of neural network\n",
    "- perform the first (coarse-level) feature extraction while reducing the feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb8da4-a5b1-43ae-8978-6389e362ae2d",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9579e74-9385-4a35-9b96-5fb38f11b06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "\n",
    "def stem(inputs):\n",
    "    \"\"\"Constructs the Stem Convolution Group\n",
    "    inputs: the input tensor\"\"\"\n",
    "    outputs = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")(\n",
    "        inputs\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fcc24-febb-4a3f-a84b-c8ccdc464d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e526d7-508c-4b9d-b5e5-cc792b6fd0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, ReLU, ZeroPadding2D\n",
    "\n",
    "\n",
    "def stem(inputs):\n",
    "    \"\"\"Constructs the Stem Convolution Group\n",
    "    inputs: the input vector\"\"\"\n",
    "    outputs = ZeroPadding2D(padding=(3, 3))(inputs)\n",
    "    outputs = Conv2D(64, (7, 7), strides=(2, 2), padding=\"valid\")(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = ReLU()(outputs)\n",
    "    outputs = ZeroPadding2D(padding=(1, 1))(outputs)\n",
    "    outputs = MaxPooling2D((3, 3), strides=(2, 2))(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2da46-8622-4b3b-b959-7f258135bcc0",
   "metadata": {},
   "source": [
    "### ResNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "201e3929-4e88-4060-8a48-ab56461d0a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stem(inputs):\n",
    "    \"\"\"Construct the Stem Convolution Group\n",
    "    inputs: input vector\n",
    "    \"\"\"\n",
    "\n",
    "    # Using padding='same' instead of ZeroPadding2D as in VGG.\n",
    "    outputs = Conv2D(64, (7, 7), strides=(2, 2), padding=\"same\")(inputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = ReLU()(outputs)\n",
    "    outputs = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbf6fb-fd90-4be9-a75a-d267c5d2c73a",
   "metadata": {},
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbcd78ca-40bc-4d6b-826c-0f79c0b2af81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stem(inputs):\n",
    "    \"\"\"Create the stem entry into the neural network\n",
    "    inputs: input tensor to neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # A 5x5 convolution refactored as two 3x2 convolutions.\n",
    "    outputs = Conv2D(32, (3, 3), strides=(2, 2))(inputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = ReLU()(outputs)\n",
    "\n",
    "    outputs = Conv2D(64, (3, 3), strides=(1, 1))(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = ReLU()(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139d3d3-4c3a-43e3-8b84-df6e2fdcdc26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-stem\n",
    "\n",
    "The purpose of pre-stem is to move into the graph (model) some or all the data preprocessing that was performed upstream.\n",
    "\n",
    "Some of functions typically performed by a pre-stem group are:\n",
    "- preprocessing\n",
    "    - adapting a model to different input size\n",
    "    - normalization\n",
    "- augmentation\n",
    "    - resizing and cropping\n",
    "    - translational and scale invariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8533e0b3-a8ef-4a93-b134-2ddc6d4b4baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "\n",
    "def prestem(input_shape):\n",
    "    \"\"\"prestem layers\"\"\"\n",
    "    outputs = Normalization(input_shape=input_shape)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "523ed219-2e59-4e9f-8aa4-d86155e88fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "wrapper_model = Sequential() # Creates an empty wrapper model.\n",
    "wrapper_model.add(prestem(input_shape=(224, 224, 3)))\n",
    "wrapper_model.add(model) # Adds the existing model to the wrapper model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802c2e3-abe9-46d2-9e2e-c62ecf50ddf0",
   "metadata": {},
   "source": [
    "## Learner Component\n",
    "\n",
    "The _learner component_ is where we generally perform feature learning through more detailed feature extraction. \n",
    "\n",
    "This process is also referred to as _representational_ or _transformational learning_.\n",
    "\n",
    "The learner component consists of one or more convolutional groups, and each group consists of one or more convolutional blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53962be5-f058-4780-9593-ed073b6e8311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learner(inputs, groups):\n",
    "    \"\"\"learner layers\n",
    "    inputs: the input tensors (feature maps)\n",
    "    groups: the block parameters for each group\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "    for group_params in groups:\n",
    "        outputs = group(outputs, **group_params)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def group(inputs, **blocks):\n",
    "    \"\"\"group layers\n",
    "    inputs: the input tensors (feature maps)\n",
    "    blocks: the block parameters for each block\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "    for block_params in blocks:\n",
    "        outputs = block(**block_params)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def block(inputs, **params):\n",
    "    \"\"\"block layers\n",
    "    inputs: the input tensor (feature maps)\n",
    "    params: the block parameters for the block\n",
    "    \"\"\"\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a454592-8631-4b25-af84-11e67f34cc1d",
   "metadata": {},
   "source": [
    "outputs = learner(outputs, [{\"n_filters\": 128}, {\"n_filters\": 128}, {\"n_filters\": 256}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63f5c6-4056-4809-b1ab-e3696423f17b",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92f182de-632a-448c-a488-98f366c60fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learner(inputs, groups):\n",
    "    \"\"\"Construct the Learner\n",
    "    inputs: input to the learner\n",
    "    groups: group parameters per group\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "    group_params = groups.pop(0)\n",
    "    outputs = group(outputs, **group_params, strides=(1, 1))\n",
    "\n",
    "    for group_params in groups:\n",
    "        outputs = group(outputs, **group_params, strides=(2, 2))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc89735e-7b93-4faa-b051-52365396a649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group(inputs, blocks, strides=(2, 2)):\n",
    "    \"\"\"Construct a Residual Group\n",
    "    inputs: input to the group\n",
    "    blocks: block parameter for each block\n",
    "    strides: whether the projection block is a strided convolution\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "    block_params = blocks.pop(0)\n",
    "    outputs = projection_block(outputs, strides=strides, **block_params)\n",
    "\n",
    "    for block_params in blocks:\n",
    "        outputs = identity_block(outputs, **block_params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ff03f-9425-4076-823e-67292fe638e6",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "\n",
    "The _learner component_ in __DenseNet__ consists of four convolutional groups. \n",
    "\n",
    "Each group, with the exception of the last group, delays pooling to the end of the group (called _transactional block_). \n",
    "\n",
    "The feature maps will be pooled and flattened by the task component, so it is redundant to pool at the end of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47f8d9c4-5c17-497b-8926-47c628ee48f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learner(inputs, groups, reduction):\n",
    "    \"\"\"Construct the Learner\n",
    "    inputs: input to the learner\n",
    "    groups: set of number of blocks per group\n",
    "    reduction: the amount to reduce (compress) feature maps by\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "    last = (\n",
    "        groups.pop()\n",
    "    )  # Pop off the last dense group parameters and saves for the end.\n",
    "\n",
    "    for group_params in groups:\n",
    "        outputs = group(outputs, reduction, **group_params)\n",
    "\n",
    "    # Adds the last group without a transactional block.\n",
    "    outputs = group(outputs, last, reduction=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cdc56c2-4ab7-4ebb-b53c-d7046b218fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group(inputs, reduction=None, **blocks):\n",
    "    \"\"\"Construct a Dense Group\n",
    "    inputs: input tensor to the group\n",
    "    reduction: amount to reduce feature map by\n",
    "    blocks: parameters for each dense block in the group\n",
    "    \"\"\"\n",
    "    outputs = inputs\n",
    "\n",
    "    for block_params in blocks:\n",
    "        outputs = residual_blocks(outputs, **block_params)\n",
    "\n",
    "    if reduction is not None:\n",
    "        outputs = transition_block(outputs, reduction)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287314c-37bc-4892-af7c-e8fcf4ecdff2",
   "metadata": {},
   "source": [
    "## Task Component\n",
    "\n",
    "The _task component_ is where we generally perform task learning. In large conventional CNNs for image classification, this component typically consists of two layers:\n",
    "\n",
    "- _bottleneck layer_ - performs dimensionality reduction of final feature maps into latent space\n",
    "- _classifier layer_ - performs the task the model is learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a918ad-a040-427c-b2d9-a40c91101da9",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bd7d390-6929-40b6-ad73-9df093d897b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classifier(inputs, n_classes):\n",
    "    \"\"\"The output classifier\n",
    "    inputs: input tensor to the classifier\n",
    "    n_classes: number of output classes\n",
    "    \"\"\"\n",
    "    # Use global average pooling to reduce and flatten the feature maps (latent space)\n",
    "    # into a 1D feature vector (bottleneck layer).\n",
    "    outputs = GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    # The fully connected dense layer for the final classification of the input.\n",
    "    outputs = Dense(n_classes, activation=\"softmax\")(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8275829-a1b3-422c-9786-8b723e21db70",
   "metadata": {},
   "source": [
    "### Multilayer output\n",
    "\n",
    "today, we build not models, but applications that are amalgamation, or composition, of models. As a result, we no longer treat the task component as a single output.\n",
    "\n",
    "Depending on how the model is connected to other models in the application, there can be up to four outputs:\n",
    "\n",
    "- Feature extraction\n",
    "    - high dimensionality (encoding)\n",
    "    - low dimensionality (embedding) - feature vector\n",
    "- Prediction\n",
    "    - prediction pre-activation (probabilities) - soft targets\n",
    "    - post-activation (outputs) - hard targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae2c4c7e-3085-49d5-80b7-ba404c396cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classifier(inputs, n_classes):\n",
    "    \"\"\"The output classifier\n",
    "    inputs: input tensor to the classifier\n",
    "    n_classes: number of output classes\n",
    "    \"\"\"\n",
    "\n",
    "    # High-dimensionality feature extraction (encoding)\n",
    "    encoding = inputs\n",
    "\n",
    "    # Low-dimensionality feature extraction (embedding).\n",
    "    embeddings = GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    # Pre-activation probabilities (soft labels)\n",
    "    probabilities = Dense(n_classes)(embeddings)\n",
    "\n",
    "    # Post-activation probabilities (hard labels)\n",
    "    outputs = Activation(\"softmax\")(outputs)\n",
    "\n",
    "    # Returns a tuple of all four outputs.\n",
    "    return encodings, embeddings, probabilities, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3e860-d48a-46da-a50b-60f5b4308e7e",
   "metadata": {},
   "source": [
    "### SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ada01dae-74ff-477a-9ef6-c0c0a22cd029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classifier(inputs, n_classes):\n",
    "    \"\"\"Construct the Classifier\n",
    "    inputs: input tensor to the classifier\n",
    "    n_classes: number of output classes\n",
    "    \"\"\"\n",
    "\n",
    "    # Sets the number of filters equal to the number of output classes.\n",
    "    encoding = Conv2D(n_classes, (1, 1), strides=1, activation=\"relu\", padding=\"same\")(\n",
    "        inputs\n",
    "    )\n",
    "\n",
    "    # Reduce each feature map (class) to a single value (soft label).\n",
    "    embedding = GlobalAveragePooling2D()(outputs)\n",
    "\n",
    "    # Use softmax to squash all the class probabilities to add up to 100%.\n",
    "    outputs = Activation(\"softmax\")(outputs)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
